# -*- coding: utf-8 -*-
"""machine learning project on stock optimisation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WTF7mR3P_3xMFgITE1CFA_m91Sq0c-Dz
"""

import pandas as pd

# Load the data from the CSV file
df = pd.read_csv('/content/tsla.us.csv',parse_dates=True)

# Assuming all columns are concatenated into a single column
# Split the single column into separate columns based on the comma delimiter
df = df['Date,Open,High,Low,Close,Volume,OpenInt'].str.split(',', expand=True)

# Rename the columns
df.columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'OpenInt']

# Convert 'Date' column to datetime type
df['Date'] = pd.to_datetime(df['Date'])

# Display the DataFrame to verify the data
print(df.head())

"""parse_dates=True parameter is used to automatically parse dates in the CSV file into datetime objects.
ince the columns are concatenated into a single string column (possibly due to how the data is structured in the CSV), str.split(',', expand=True) is applied to split this single column into separate columns based on the comma delimiter.
expand=True ensures that the split results are expanded into separate columns.

dateobject -> string -> date
"""





# import pandas_datareader.data as web
# import datetime as dt

"""**pandas_datareader.data:** This module is used to extract data from various online sources into a pandas DataFrame. It provides a convenient way to access data from sources such as Yahoo Finance, Google Finance, World Bank, and many others. The DataReader function is typically used to retrieve data.

**datetime:** This module provides classes for manipulating dates and times. It includes classes like datetime for representing dates and times, timedelta for representing differences between two dates or times, and date for representing dates. It is commonly used in conjunction with pandas_datareader.data to specify the date range for which data should be retrieved.
"""

# pip install yfinance

"""**The yfinance library is a Python package that provides a convenient way to download historical market data from Yahoo Finance. It allows you to retrieve stock price data, dividend data, and other financial data for analysis and research purposes.**"""

# this will download the historic data of apple from yfinance library from a specific date interval
# import yfinance as yf
# stock_symbol = 'AAPL'
# start_date = '2022-01-01'
# end_date = '2022-12-31'

# # Download historical market data
# stock_data = yf.download(stock_symbol, start=start_date, end=end_date)

# # Print the first few rows of the data
# print(stock_data.head())

# df.columns

# df.shape

"""data visualtization"""

from matplotlib import style

style.use('ggplot')

"""In pd.read_csv, the parse_dates parameter is used to specify the columns that should be parsed as datetime objects. This can be helpful when you have date/time information in your CSV file that you want to be recognized as datetime objects in your DataFrame.

The index_col parameter is used to specify which column should be used as the index of the DataFrame. By default, DataFrame indices are numbered starting from 0, but you can specify one or more columns to be used as the index instead.
"""

df.columns
df.shape

"""Converts the 'Volume' column to numeric data type (float64), ensuring any non-numeric values are coerced to NaN (Not a Number)."""

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

# Assuming df is your DataFrame with the 'Volume' column
# Coerce 'Volume' column to numeric, handling errors by setting them to NaN
df['Volume'] = pd.to_numeric(df['Volume'], errors='coerce')

# Plot the 'Volume' column with formatted date axis
plt.plot(df.index, df['Volume'])

# Set date format on x-axis
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

# Rotate date labels for better readability
plt.xticks(rotation=45)

# Add labels and title
plt.xlabel('Date')
plt.ylabel('Volume')
plt.title('Volume Over Time')

# Display the plot
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

# Assuming df is your DataFrame with the 'Close' column
# Coerce 'Close' column to numeric, handling errors by setting them to NaN
df['Close'] = pd.to_numeric(df['Close'], errors='coerce')

# Plot the 'Close' column with formatted date axis
plt.plot(df.index, df['Close'])

# Set date format on x-axis
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

# Rotate date labels for better readability
plt.xticks(rotation=45)

# Add labels and title
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.title('Close Price Over Time')

# Display the plot
plt.show()

df["100ma"]=df["Close"].rolling(window=100,min_periods=0).mean()

df

!pip install mpl_finance

import matplotlib.pyplot as plt
from mpl_finance import candlestick_ohlc
import matplotlib.dates as mdates
df['Date'] = pd.to_datetime(df['Date'])  # Convert 'Date' column to datetime type
df.set_index('Date', inplace=True)  # Set 'Date' column as index

df_ohlc = df['Close'].resample('10D').ohlc()
df_volume = df['Volume'].resample('10D').sum()

df_ohlc.reset_index(inplace=True)  # Resetting index to make 'Date' a column

df_ohlc['Date'] = df_ohlc['Date'].map(mdates.date2num)

ax1 = plt.subplot2grid((6,1), (0,0), rowspan=5, colspan=1)
ax2 = plt.subplot2grid((6,1), (5,0), rowspan=1, colspan=1, sharex=ax1)

ax1.xaxis_date()

candlestick_ohlc(ax1, df_ohlc.values, width=2, colorup='g')  # 'candlestick_ohlc' instead of 'candlestick ohlc'

ax2.fill_between(df_volume.index.map(mdates.date2num), df_volume.values, 0)  # Corrected fill_between syntax

plt.show()

"""Importing Libraries:

import bs4 as bs: Imports the BeautifulSoup library, commonly used for web scraping tasks, and aliases it as bs.
import pickle: Imports the pickle module, which is used for serializing and deserializing Python objects.
import requests: Imports the requests library, which is used to make HTTP requests to web servers.
Function Definition:

def save_sp500_tickers():: Defines a function named save_sp500_tickers. This function will scrape the tickers of companies in the S&P 500 index from a Wikipedia page and save them into a pickle file.
HTTP Request:

resp = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'): Sends an HTTP GET request to the specified URL (Wikipedia page listing S&P 500 companies) and stores the response in the variable resp.
Parsing HTML:

soup = bs.BeautifulSoup(resp.text, 'html.parser'): Parses the HTML content of the response using BeautifulSoup and stores the result in the variable soup. This allows us to navigate and extract data from the HTML structure.
Finding the Table:

table = soup.find('table', {'class': 'wikitable sortable'}): Finds the table containing the list of S&P 500 companies by searching for the HTML <table> element with the specified class attribute.
Scraping Tickers:

for row in table.findAll('tr')[1:]:: Iterates over each row (<tr>) in the table, excluding the header row (hence [1:]).
ticker = row.findAll('td')[0].text.strip(): Finds the first cell (<td>) in each row, which contains the ticker symbol, and extracts the text. The .strip() method is used to remove any leading or trailing whitespace.
tickers.append(ticker): Appends the ticker symbol to the list of tickers.
Saving Data:

with open("sp500tickers.pickle", "wb") as f:: Opens a file named sp500tickers.pickle in binary write mode.
pickle.dump(tickers, f): Serializes the list of tickers using the pickle.dump() method and writes it to the file.
Returning Data:

return tickers: Returns the list of tickers after scraping and saving them.
Function Call:

tickers = save_sp500_tickers(): Calls the save_sp500_tickers() function and stores the returned list of tickers in the variable tickers.
Printing Data:

print(tickers): Prints the list of tickers to the console.
"""

import bs4 as bs
import pickle
import requests

def save_sp500_tickers():
    resp = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')
    soup = bs.BeautifulSoup(resp.text, 'html.parser')
    table = soup.find('table', {'class': 'wikitable sortable'})
    tickers = []

    for row in table.findAll('tr')[1:]:
        ticker = row.findAll('td')[0].text.strip()  # Strip to remove extra whitespace
        tickers.append(ticker)

    with open("sp500tickers.pickle", "wb") as f:
        pickle.dump(tickers, f)

    return tickers

tickers = save_sp500_tickers()
print(tickers)

pip install fix_yahoo_finance

!pip install yfinance
!pip install requests

import yfinance as yf
from pandas_datareader import data as pdr
import datetime as dt
import os
import pickle
import requests
from bs4 import BeautifulSoup as bs

def download(tickers, start, end):
    data = pdr.get_data_yahoo(tickers, start=start, end=end)
    return data

yf.pdr_override()

def save_sp500_tickers():
    resp = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')
    soup = bs(resp.text, 'lxml')
    table = soup.find('table', {'class': 'wikitable sortable'})
    tickers = []
    for row in table.findAll('tr')[1:]:
        ticker = row.findAll('td')[0].text.strip()
        ticker = str(ticker).replace('.', '-')  # Replace dots in ticker symbols
        tickers.append(ticker)
    with open("sp500tickers.pickle", "wb") as f:
        pickle.dump(tickers, f)
    return tickers

def get_data_from_yahoo(reload_sp500=False):
    tickers = None
    if reload_sp500:
        tickers = save_sp500_tickers()
    else:
        with open("sp500tickers.pickle", "rb") as f:
            tickers = pickle.load(f)

    if tickers is None:
        print("Error: Failed to load S&P 500 tickers.")
        return

    if not os.path.exists('stock_dfs'):
        os.makedirs('stock_dfs')

    start = dt.datetime(2000, 1, 1)
    end = dt.datetime(2016, 12, 31)

    for ticker in tickers:
        csv_file_path = f'stock_dfs/{ticker}.csv'
        if not os.path.exists(csv_file_path):
            try:
                df = pdr.get_data_yahoo(ticker, start, end)
                df.to_csv(csv_file_path)
                print(f'Data saved for {ticker}')
            except Exception as e:
                print(f"Error fetching data for {ticker}: {e}")
        else:
            print(f'Already have data for {ticker}')

# Example usage:
get_data_from_yahoo(reload_sp500=True)

"""**This Python code defines a function compile_data that reads stock market data from CSV files for various tickers (stock symbols) stored in a pickle file (sp500tickers.pickle). It then creates a pandas DataFrame (main_df) to store the adjusted closing prices for each ticker, joining them together into a single DataFrame. The function iterates over the tickers, reads the corresponding CSV files, sets the index to the 'Date' column, renames the 'Adj Close' column to the ticker symbol, and drops unnecessary columns ('Open', 'High', 'Low', 'Close', 'Volume'). Every 10 iterations, it prints the current count. Finally, it prints the head of the main_df DataFrame and saves it to a CSV file (sp500_joined_closes.csv). The function is then called at the end to execute this data compilation process.**"""

import pandas as pd
import pickle

def compile_data():
    with open("sp500tickers.pickle", "rb") as f:
        tickers = pickle.load(f)

    main_df = pd.DataFrame()

    for count, ticker in enumerate(tickers):
        df = pd.read_csv(f'stock_dfs/{ticker}.csv')
        df.set_index('Date', inplace=True)
        df.rename(columns={'Adj Close': ticker}, inplace=True)
        df.drop(['Open', 'High', 'Low', 'Close', 'Volume'], axis=1, inplace=True)

        if main_df.empty:
            main_df = df
        else:
            main_df = main_df.join(df, how='outer')

        if count % 10 == 0:
            print(count)

    print(main_df.head())
    main_df.to_csv('sp500_joined_closes.csv')

compile_data()

"""Importing Libraries:

The code starts by importing the necessary libraries:
pandas (as pd): Used for data manipulation and analysis.
numpy (as np): Used for numerical computations.
matplotlib.pyplot (as plt): Used for data visualization.
Defining the Function:

The function visualize_data() is defined. This function will encapsulate the entire process of visualizing the correlation matrix.
Reading Data:

The code reads the stock data from the CSV file "sp500_joined_closes.csv" into a pandas DataFrame df.
Calculating Correlation Matrix:

The code calculates the correlation matrix of the DataFrame df using the .corr() method. This matrix represents the pairwise correlations between all the columns (stocks) in the DataFrame.
Preparing Data for Visualization:

The correlation matrix is converted into a NumPy array data, which will be used for plotting.
Creating the Plot:

A new figure and axis are created for the heatmap using plt.figure() and fig.add_subplot(1, 1, 1).
Plotting the Heatmap:

The pcolor() function from matplotlib is used to create the heatmap. The correlation values in the data array are used to determine the colors of the heatmap cells.
Adding Colorbar:

A colorbar is added to the plot using fig.colorbar(heatmap). The colorbar indicates the range of correlation values.
Setting Axis Ticks and Labels:

The ticks and labels for both the x-axis and y-axis are set to correspond to the stock symbols (column names) using ax.set_xticks() and ax.set_yticks().
Customizing Axis Labels:

The labels for the x-axis are rotated by 90 degrees to improve readability using ax.set_xticklabels().
Setting Color Limits:

The color limits for the heatmap are set to -1 and 1 using heatmap.set_clim(-1, 1). This ensures that the colors represent the full range of correlation values (-1 to 1).
Adjusting Layout and Displaying Plot:

plt.tight_layout() is called to automatically adjust subplot parameters to fit the plot into the figure area without overlapping.
Finally, plt.show() displays the plot on the screen.
Function Call:

The visualize_data() function is called at the end to execute the visualization process.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def visualize_data():
    df = pd.read_csv("sp500_joined_closes.csv")

    df_corr = df.corr()
    data = df_corr.values

    fig = plt.figure()
    ax = fig.add_subplot(1, 1, 1)

    heatmap = ax.pcolor(data, cmap=plt.cm.RdYlGn)
    fig.colorbar(heatmap)

    ax.set_xticks(np.arange(data.shape[0]) + 0.5, minor=False)
    ax.set_yticks(np.arange(data.shape[1]) + 0.5, minor=False)
    ax.invert_yaxis()
    ax.xaxis.tick_top()

    column_labels = df_corr.columns
    row_labels = df_corr.index

    ax.set_xticklabels(column_labels, rotation=90)
    ax.set_yticklabels(row_labels)

    heatmap.set_clim(-1, 1)
    plt.tight_layout()
    plt.show()

visualize_data()

"""This code defines a function process_data_for_labels that takes a stock ticker symbol as input, reads a CSV file (sp500_joined_closes.csv) containing stock price data, and processes the data to create labels for machine learning classification or regression tasks. It calculates the percentage change in stock price ({}_{}d.format(ticker, i)) for the next 1 to 7 days (hm_days) and adds these columns to the DataFrame (df). It fills any NaN values with 0. The function returns the list of tickers and the processed DataFrame.

The code then calls the process_data_for_labels function with the ticker symbol 'XOM' and prints the first few rows of the processed DataFrame.

"""

import numpy as np
import pandas as pd
import pickle

def process_data_for_labels(ticker):
    hm_days = 7  # corrected the variable name

    df = pd.read_csv('sp500_joined_closes.csv', index_col=0)
    tickers = df.columns.values.tolist()
    df.fillna(0, inplace=True)

    for i in range(1, hm_days + 1):  # corrected indentation
        df['{}_{}d'.format(ticker, i)] = (df[ticker].shift(-i) - df[ticker]) / df[ticker]  # corrected syntax error
        df.fillna(0, inplace=True)

    return tickers, df

tickers, df = process_data_for_labels('XOM')
print(df.head())  # Example usage: printing the first few rows of the processed DataFrame

def buy_sell_hold(*args):
    cols = [c for c in args]
    requirement = 0.02
    recommendations = []

    for col in cols:
        if col > requirement:
            recommendations.append(1)  # Buy recommendation
        elif col < -requirement:
            recommendations.append(-1)  # Sell recommendation
        else:
            recommendations.append(0)  # Hold recommendation

    return recommendations

"""This code processes stock market data to create features for a machine learning model that predicts whether to buy, sell, or hold a stock. Here's a brief explanation:

process_data_for_labels(ticker): Reads stock market data from a CSV file (sp500_joined_closes.csv) and calculates the percentage change in stock price for the next 1 to 7 days for a given stock ticker. It returns the list of tickers and a DataFrame with the processed data.

extract_featuresets(ticker): Uses the process_data_for_labels function to get the tickers and processed DataFrame. It then creates a target column ({}_target) by applying a function (buy_sell_hold) to the 1 to 7-day percentage change columns. It also handles missing values, calculates percentage changes for all tickers, and prepares the features (X) and labels (y) for the machine learning model. Finally, it returns X, y, and the updated DataFrame.

The code demonstrates a basic pipeline for preparing stock market data for machine learning analysis, including feature extraction and target labeling.

Import Statements:

import numpy as np: Imports the NumPy library and gives it the alias np.
import pandas as pd: Imports the Pandas library and gives it the alias pd.
from collections import Counter: Imports the Counter class from the collections module.
Function Definition:

def extract_featuresets(ticker):: Defines a function named extract_featuresets that takes a parameter named ticker.
Data Retrieval:

tickers, df = process_data_for_labels(ticker): Calls a function named process_data_for_labels to retrieve stock data for the specified ticker. It assigns the returned tickers list to tickers and the DataFrame containing the data to df.
Target Variable Generation:

df['{}_target'.format(ticker)] = list(map(buy_sell_hold, ...)): Generates a target variable for the given ticker by calling the buy_sell_hold function on the 1-day to 7-day percentage change values of the stock price. The result is stored in a new column named {ticker}_target in the DataFrame.
Data Spread Analysis:

str_vals = [str(i) for i in df['{}_target'.format(ticker)].values.tolist()]: Converts the target variable values to strings.
print('Data spread:', Counter(str_vals)): Prints the distribution of target variable values using the Counter class. This provides insights into how the data is spread across different categories.
Data Cleaning:

df.fillna(0, inplace=True): Fills missing values in the DataFrame with zeros.
df.replace((np.inf, -np.inf), np.nan, inplace=True): Replaces infinite values with NaN in the DataFrame.
df.dropna(inplace=True): Drops rows with missing values from the DataFrame.
Feature Extraction:

df_vals = df[[ticker for ticker in tickers]].pct_change(): Calculates the percentage change for each stock in the DataFrame using the pct_change() method. This represents the feature set for training the model.
Feature and Target Variable Assignment:

X = df_vals.values: Assigns the percentage change values to the feature variable X.
y = df['{}_target'.format(ticker)].values: Assigns the target variable values to y.
Return Statement:

return X, y, df: Returns the feature variable X, target variable y, and the modified DataFrame df.
Function Call:

extract_featuresets('XOM'): Calls the extract_featuresets function with the ticker symbol 'XOM' to process data for Exxon Mobil Corporation.
"""

import numpy as np
def process_data_for_labels(ticker):
    hm_days = 7
    df = pd.read_csv('sp500_joined_closes.csv', index_col=0)
    tickers = df.columns.values.tolist()
    df.fillna(0, inplace=True)

    for i in range(1, hm_days+1):
        df['{}_{}d'.format(ticker, i)] = (df[ticker].shift(-i) - df[ticker]) / df[ticker]

    df.fillna(0, inplace=True)
    return tickers, df

def buy_sell_hold(*args):
    cols = [c for c in args]
    requirement = 0.02
    for col in cols:
        if col > 0.029:
            return 1
        if col < -0.027:
            return -1
    return 0

def extract_featuresets(ticker):
    tickers, df = process_data_for_labels(ticker)

    df['{}_target'.format(ticker)] = list(map( buy_sell_hold,
       df['{}_1d'.format(ticker)],
       df['{}_1d'.format(ticker)],
       df['{}_2d'.format(ticker)],
       df['{}_3d'.format(ticker)],
       df['{}_4d'.format(ticker)],
       df['{}_5d'.format(ticker)],
       df['{}_6d'.format(ticker)],
       df['{}_7d'.format(ticker)]))

    vals = df['{}_target'.format(ticker)].values.tolist()
    str_vals = [str(i) for i in vals]
    print('Data spread:', Counter(str_vals))

    df.fillna(0, inplace=True)
    df = df.replace([np.inf, -np.inf], np.nan)
    df.dropna(inplace=True)

    df_vals = df[[ticker for ticker in tickers]].pct_change()
    df_vals = df_vals.replace([np.inf, -np.inf], 0)
    df_vals.fillna(0, inplace=True)

    X = df_vals.values
    y = df['{}_target'.format(ticker)].values

    return X, y, df

extract_featuresets("XOM")



pip install scikit-learn

from collections import Counter
import numpy as np
import pandas as pd
import pickle

from sklearn.model_selection import train_test_split
from sklearn.ensemble import VotingClassifier, RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import LinearSVC

def process_data_for_labels(ticker):
    hm_days = 7
    df = pd.read_csv('sp500_joined_closes.csv', index_col=0)
    tickers = df.columns.values.tolist()
    df.fillna(0, inplace=True)

    for i in range(1, hm_days+1):
        df['{}_{}d'.format(ticker, i)] = (df[ticker].shift(-i) - df[ticker]) / df[ticker]

    df.fillna(0, inplace=True)
    return tickers, df

def buy_sell_hold(*args):
    cols = [c for c in args]
    requirement = 0.02
    for col in cols:
        if col > 0.029:
            return 1
        if col < -0.027:
            return -1
    return 0

def extract_featuresets(ticker):
    tickers, df = process_data_for_labels(ticker)

    df['{}_target'.format(ticker)] = list(map( buy_sell_hold,
       df['{}_1d'.format(ticker)],
       df['{}_1d'.format(ticker)],
       df['{}_2d'.format(ticker)],
       df['{}_3d'.format(ticker)],
       df['{}_4d'.format(ticker)],
       df['{}_5d'.format(ticker)],
       df['{}_6d'.format(ticker)],
       df['{}_7d'.format(ticker)]))

    vals = df['{}_target'.format(ticker)].values.tolist()
    str_vals = [str(i) for i in vals]
    print('Data spread:', Counter(str_vals))

    df.fillna(0, inplace=True)
    df = df.replace([np.inf, -np.inf], np.nan)
    df.dropna(inplace=True)

    df_vals = df[[ticker for ticker in tickers]].pct_change()
    df_vals = df_vals.replace([np.inf, -np.inf], 0)
    df_vals.fillna(0, inplace=True)

    X = df_vals.values
    y = df['{}_target'.format(ticker)].values

    return X, y, df

def do_ml(ticker):
    X, y, df = extract_featuresets(ticker)

    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25)

    # clf = KNeighborsClassifier()
    clf = VotingClassifier([('lsvc', LinearSVC()),
                            ('knm', KNeighborsClassifier()),
                            ('rfor', RandomForestClassifier())])


    clf.fit(X_train, y_train)
    confidence = clf.score(X_test, y_test)
    print('Accuracy', confidence)
    predictions = clf.predict(X_test)
    print('Predicted spread:', Counter(predictions))

    return confidence

do_ml('BAC')



from textblob import TextBlob

# Example tweet
tweet = "Stocks are looking good today!"

# Perform sentiment analysis
blob = TextBlob(tweet)
sentiment = blob.sentiment

# Print sentiment
print(sentiment)